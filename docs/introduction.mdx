# Multi-Task NLP Analysis: Detecting Emotions, Hate Speech & Violence in Text

## Project Overview

This project implements a multi-task deep learning model that can simultaneously analyze text for three distinct tasks:

1. **Emotion Detection**: Identifies the emotional tone in text (Sadness, Joy, Love, Anger, Fear, Surprise)
2. **Violence Detection**: Classifies text for different types of violence (Physical Violence, Sexual Violence, Emotional Violence, Economic Violence, Harmful Traditional Practice)
3. **Hate Speech Detection**: Detects hate speech in text (Hate Speech, Offensive Speech, Neither)

By leveraging a shared architecture, the model efficiently learns common language features across all three tasks while maintaining task-specific output layers for specialized predictions.

## Data Sources

The project uses three distinct datasets:

1. **Emotions Dataset**: Contains text labeled with six emotions (Sadness, Joy, Love, Anger, Fear, Surprise)
2. **Violence Dataset**: Text samples labeled with different types of violence (Physical, Sexual, Emotional, Economic, and Harmful Traditional Practices)
3. **Hate Speech Dataset**: Text samples classified as Hate Speech, Offensive Speech, or Neither

## Project Workflow

### 1. Data Preparation

#### Data Loading
First, we load the three datasets and ensure they have a consistent structure with text and label columns.

#### Data Cleaning
We clean the datasets by:
- Removing unwanted columns
- Renaming columns for consistency
- Checking for and handling missing values
- Converting label encodings to a consistent format

### 2. Text Preprocessing

Text preprocessing is a critical step that transforms raw text into a format suitable for machine learning:

- **Lowercasing**: Converting all text to lowercase to ensure consistency
- **Removing Special Characters**: URLs, mentions, hashtags, and numbers are removed
- **Removing Punctuation**: Punctuation marks are stripped from the text
- **Tokenization**: Breaking text into individual words or tokens
- **Removing Stopwords**: Eliminating common words that typically don't contribute much meaning
- **Lemmatization**: Reducing words to their base or root form to normalize vocabulary

### 3. Sequence Processing

To feed text data into neural networks, we need to convert words into numerical representations:

- **Tokenization**: Creating a vocabulary mapping each unique word to a numerical index
- **Converting to Sequences**: Transforming text into sequences of numerical indices
- **Padding**: Ensuring all sequences have the same length by padding shorter sequences

### 4. Data Splitting

We divide each dataset into:
- **Training set (80%)**: Used to train the model
- **Validation set (20%)**: Used to evaluate the model during training and tune hyperparameters

### 5. Model Architecture

The multi-task model uses a shared-private architecture:

- **Shared Layers**: Common components that learn general text features
  - **Embedding Layer**: Converts word indices into dense vector representations
  - **LSTM Layer**: Processes sequential data and captures dependencies between words
  
- **Task-Specific Layers**: Specialized components for each classification task
  - **Global Pooling**: Extracts the most important features from sequences
  - **Dense Layers**: Task-specific output layers that predict the final classifications

### 6. Model Training

The model is trained with:
- **Multi-task Loss Function**: Combines losses from all three tasks
- **Adam Optimizer**: Efficiently updates model weights
- **Callbacks**: Early stopping, learning rate reduction, and model checkpointing

### 7. Evaluation

We evaluate the model's performance using:
- **Accuracy**: Overall correctness of predictions
- **Precision, Recall, F1-score**: Detailed metrics for each class in each task
- **Confusion Matrices**: Visual representation of model predictions

### 8. Making Predictions

Finally, we create a function to make predictions on new text inputs, allowing the model to simultaneously analyze text for emotions, violence, and hate speech.

## Technical Concepts Explained

### Natural Language Processing (NLP)

NLP is a field of artificial intelligence that focuses on the interaction between computers and human language. It enables computers to understand, interpret, and generate human language in a way that is both meaningful and useful.

### Long Short-Term Memory (LSTM)

LSTM is a specialized type of Recurrent Neural Network (RNN) designed to overcome the vanishing gradient problem that standard RNNs face. LSTMs include a memory cell that can maintain information for long periods, making them particularly useful for processing sequential data like text.

Key components of an LSTM cell:
- **Input Gate**: Controls what new information is stored in the cell state
- **Forget Gate**: Decides what information to discard from the cell state
- **Output Gate**: Determines what the next hidden state should be

LSTMs are effective for text analysis because they can capture long-range dependencies between words in a sentence or document.

### Stopwords

Stopwords are common words in a language that typically don't contribute significant meaning to the text (e.g., "the", "is", "and", "of"). Removing stopwords reduces noise in the data and helps the model focus on the most meaningful content.

### Lemmatization

Lemmatization is the process of reducing words to their base or dictionary form (lemma). For example, "running" becomes "run", and "better" becomes "good". Unlike stemming (which simply chops off prefixes and suffixes), lemmatization considers the context and part of speech to convert words to their proper root form.

### Word Embeddings

Word embeddings are dense vector representations of words in a continuous vector space. They map words to vectors of real numbers such that words with similar meanings are located close to each other in the vector space. Key properties:

- **Dimensionality Reduction**: Converts sparse one-hot encoded word vectors into dense, lower-dimensional vectors
- **Semantic Relationships**: Captures relationships between words (e.g., "king" - "man" + "woman" â‰ˆ "queen")
- **Transfer Learning**: Pre-trained embeddings can transfer knowledge from large corpora

In our model, we use a trainable embedding layer that learns word representations specific to our tasks.

### Tokenization

Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be words, characters, or subwords. In NLP, tokenization is typically the first step in text processing, converting unstructured text into a format that can be easily parsed and analyzed.

### Padding

Neural networks require fixed-length inputs, but text sequences naturally vary in length. Padding solves this by:
- Adding zeros (or other placeholder values) to shorter sequences
- Truncating longer sequences to a maximum length
This ensures all inputs have the same dimensions, making batch processing possible.

### Multi-Task Learning

Multi-task learning is a machine learning approach where a single model is trained to perform multiple related tasks simultaneously. Benefits include:

- **Improved Data Efficiency**: Learning shared representations from multiple tasks
- **Reduced Overfitting**: The model learns more general features that work across tasks
- **Faster Training**: Training one model for multiple tasks is typically more efficient than training separate models

In our case, the model learns to detect emotions, violence, and hate speech using shared layers while maintaining task-specific output layers.

## Conclusion

This multi-task NLP model demonstrates how a shared architecture can effectively tackle several text classification tasks simultaneously. The approach leverages common language patterns while still accommodating the unique aspects of each task, resulting in an efficient and powerful text analysis system.

The model can be used in various applications, such as content moderation, social media monitoring, customer feedback analysis, or any scenario where understanding the emotional tone and potentially harmful content in text is valuable.
